{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Извлечение ключевых слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Корпус\n",
    "\n",
    "В корпус вошли пять коротких новостей с сайта газеты \"Завтра\" (http://zavtra.ru/). Ключевые слова представлены на сайте в виде тегов. (К сожалению, не знаю, как именно они расставляют теги).\n",
    "\n",
    "В строке с ключевыми словами уже есть ключевые слова, размеченные мной вручную (после символа \";\").\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "corpus = [i in os.listdir(\"corpus\") if not i.endswith('.conllu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = {}\n",
    "for fn in corpus:\n",
    "    meta_info = {}\n",
    "    with open(os.path.join(\"corpus\", fn), 'r', encoding='utf-8') as f:\n",
    "        meta = f.readlines()[:3]\n",
    "        for line in meta:\n",
    "            title, info = line.strip('\\n').split('\\t')\n",
    "            meta_info[title] = info\n",
    "    tags[fn] = meta_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравнение моей разметки и исходной\n",
    "\n",
    "Так себе сходимся, особенно на совсем маленьких текстах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.txt\n",
      "Есть и у меня, и в исходной: {'музыка'}\n",
      "Есть в исходной, нет в моей: {'школа', 'минкульт'}\n",
      "Нет в моей, есть в исходной: {'норматив', 'школьник', 'министерство культуры'}\n",
      "2.txt\n",
      "Есть и у меня, и в исходной: {'вера', 'выставка', 'художник'}\n",
      "Есть в исходной, нет в моей: {'русская история', 'победа', 'живопись', 'родина', 'москва'}\n",
      "Нет в моей, есть в исходной: {'союз художников', 'Россия'}\n",
      "3.txt\n",
      "Есть и у меня, и в исходной: {'концерт'}\n",
      "Есть в исходной, нет в моей: {'рок', 'музыка'}\n",
      "Нет в моей, есть в исходной: {'втб арена', 'scorpions'}\n",
      "4.txt\n",
      "Есть и у меня, и в исходной: {'сердюков', 'васильева', 'выставка', 'современное искусство'}\n",
      "Есть в исходной, нет в моей: set()\n",
      "Нет в моей, есть в исходной: {'художник', 'EVA', 'музей востока', 'министерство обороны', 'культура'}\n",
      "5.txt\n",
      "Есть и у меня, и в исходной: {'экономика', 'поколение', 'миллениалы', 'сша', 'здоровье'}\n",
      "Есть в исходной, нет в моей: {'суицид'}\n",
      "Нет в моей, есть в исходной: {'молодой возраст'}\n"
     ]
    }
   ],
   "source": [
    "all_true_their = []\n",
    "all_true_mine = []\n",
    "for inf in tags:\n",
    "    print(inf)\n",
    "    before, after = tags[inf]['Ключевые слова'].split(';')\n",
    "    their = set(before.split(', '))\n",
    "    all_true_their.append(their)\n",
    "    \n",
    "    mine = set(after.split(', '))\n",
    "    all_true_mine.append(mine)\n",
    "    print(\"Есть и у меня, и в исходной:\", their.intersection(mine))\n",
    "    print(\"Есть в исходной, нет в моей:\", their - mine)\n",
    "    print(\"Нет в моей, есть в исходной:\", mine - their)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разметка корпуса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm = \"\"\"C:\\\\Users\\\\Eiko\\\\PycharmProjects\\\\NLP_19_20\\\\udpipe-1.2.0-bin\\\\bin-win64\\\\udpipe.exe --input horizontal --output conllu \\\n",
    "--tokenize --tag --parse \\\n",
    "C:\\\\Users\\\\Eiko\\\\PycharmProjects\\\\NLP_19_20\\\\russian-syntagrus-ud-2.4-190531.udpipe \\\n",
    "< C:\\\\Users\\\\Eiko\\\\PycharmProjects\\\\NLP_19_20\\\\hw1_key_words\\\\corpus\\{} > C:\\\\Users\\\\Eiko\\\\PycharmProjects\\\\NLP_19_20\\\\hw1_key_words\\\\corpus\\{}.conllu\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: \n",
      "Loading UDPipe model: done.\n",
      "\n",
      "\n",
      "Output: \n",
      "Loading UDPipe model: done.\n",
      "\n",
      "\n",
      "Output: \n",
      "Loading UDPipe model: done.\n",
      "\n",
      "\n",
      "Output: \n",
      "Loading UDPipe model: done.\n",
      "\n",
      "\n",
      "Output: \n",
      "Loading UDPipe model: done.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "for filename in corpus:\n",
    "    commandline = comm.format(filename, filename[:-4])\n",
    "    try:\n",
    "        output = subprocess.check_output(\n",
    "            commandline, stderr=subprocess.STDOUT, shell=True, timeout=1000,\n",
    "            universal_newlines=True)\n",
    "    except subprocess.CalledProcessError as exc:\n",
    "        print(\"Status : FAIL\", exc.returncode, exc.output)\n",
    "    else:\n",
    "        print(\"Output: \\n{}\\n\".format(output))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Извлечение ключевых слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Препроцессинг: лемматизация текстов\n",
    "\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "\n",
    "def preprocessing(input_text, del_stopwords=False, del_digit=True):\n",
    "    \"\"\"\n",
    "    :input: raw text\n",
    "        1. lowercase, del punctuation, tokenize\n",
    "        2. normal form\n",
    "        3. del stopwords\n",
    "        4. del digits\n",
    "    :return: lemmas\n",
    "    \"\"\"\n",
    "    if del_stopwords:\n",
    "        russian_stopwords = set(stopwords.words('russian'))\n",
    "    words = [x.lower().strip(string.punctuation+'»«–…') for x in word_tokenize(input_text)]\n",
    "    lemmas = [morph.parse(x)[0].normal_form for x in words if x]\n",
    "\n",
    "    lemmas_arr = []\n",
    "    for lemma in lemmas:\n",
    "        if del_stopwords:\n",
    "            if lemma in russian_stopwords:\n",
    "                continue\n",
    "        if del_digit:\n",
    "            if lemma.isdigit():\n",
    "                continue\n",
    "        lemmas_arr.append(lemma)\n",
    "    return lemmas_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'в'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph.parse('в')[0].normal_form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clear_texts(corpus, del_stopwords=True):\n",
    "    for fn in corpus:\n",
    "        with open(os.path.join(\"corpus\", fn), 'r', encoding='utf-8') as f:\n",
    "            text = '\\n'.join(f.readlines()[3:])\n",
    "            yield ' '.join(preprocessing(text, del_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidfvect = TfidfVectorizer()\n",
    "tf_matrix = tfidfvect.fit_transform(get_clear_texts(corpus)).T.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(883, 5)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "matrix = pd.DataFrame(data=tf_matrix, index=tfidfvect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Номер текста:  0\n",
      "['queen', 'произведение', 'all', 'is', 'jailhouse']\n",
      "Номер текста:  1\n",
      "['художник', 'русский', 'победа', 'это', 'человек']\n",
      "Номер текста:  2\n",
      "['scorpions', 'арен', 'втб', 'задержать', 'концерт']\n",
      "Номер текста:  3\n",
      "['выставка', 'искусство', 'eva', 'творчество', 'год']\n",
      "Номер текста:  4\n",
      "['миллениал', 'поколение', 'год', 'возраст', 'рост']\n"
     ]
    }
   ],
   "source": [
    "tfidf_result = []\n",
    "for doc in matrix:\n",
    "    print(\"Номер текста: \", doc)\n",
    "    res = list(matrix[doc].nlargest(5).index)\n",
    "    tfidf_result.append(set(res))\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кажется, что на более-менее длинных текстах неплохо работает - третий текст почти идеальный. Тем не менее, для такого маленького корпуса использовать этот способ довольно бессмысленно, так как на первый план выйдут просто уникальные для корпуса слова в тексте. Это заметно по первому тексту — его ключевые слова все в латинице, потому что в других текстах таких слов просто не было."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import RAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "rake = RAKE.Rake(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Номер текста:  0\n",
      "[('перечень музыкальный произведение', 9.0), ('великий отечественный война', 9.0), ('любить высоцкий марионетка', 9.0), ('культурный норматив школьник', 9.0), ('подрастать поколение макар', 9.0)]\n",
      "Номер текста:  1\n",
      "[('духовный основа который', 9.0), ('совместный сбережение насыщение', 9.0), ('глобальный смысл искусство', 8.5), ('единство наш победа', 8.5), ('это смысл живопись', 8.0)]\n",
      "Номер текста:  2\n",
      "[('московский стадион втб-арена', 14.5), ('слушать скорп дурацкий', 9.0), ('свой заявление умудриться', 9.0), ('вторник выступить', 4.0), ('добиться заголовок', 4.0)]\n",
      "Номер текста:  3\n",
      "[('фонд русский меценат', 9.0), ('широкий круг зритель', 9.0), ('оборонный ведомство девочка', 9.0), ('несколько месяц пошлый', 9.0), ('колония общий режим', 9.0)]\n",
      "Номер текста:  4\n",
      "[('рубеж тысячелетие доказать', 9.0), ('угроза общественный благополучие', 9.0), ('центр экономический рост', 9.0), ('ближний год стоимость', 9.0), ('год миллениал чаща', 9.0)]\n"
     ]
    }
   ],
   "source": [
    "rake_result = []\n",
    "for _id, text in enumerate(get_clear_texts(corpus, del_stopwords=False)):\n",
    "    print(\"Номер текста: \", _id)\n",
    "    result = rake.run(normalize_text_simple_word_tokenize(text), maxWords=3, minFrequency=1)\n",
    "    rake_result.append({i[0] for i in result})\n",
    "    print(sorted(result, key=lambda x: x[1], reverse=True)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Извлекаются так себе. Попробуем другой токенизатор?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "from pymorphy2.tokenizers import simple_word_tokenize\n",
    "m = MorphAnalyzer()\n",
    "def normalize_text_simple_word_tokenize(text):\n",
    "    lemmas = []\n",
    "    for t in simple_word_tokenize(text):\n",
    "        lemmas.append(\n",
    "            m.parse(t)[0].normal_form\n",
    "        )\n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for fn in corpus:\n",
    "        with open(os.path.join(\"corpus\", fn), 'r', encoding='utf-8') as f:\n",
    "            text = '\\n'.join(f.readlines()[3:])\n",
    "            texts.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Номер текста:  0\n",
      "[('перечень музыкальный произведение', 9.0), ('великий отечественный война', 9.0), ('культурный норматив школьник', 9.0), ('подрастать поколение макаре', 9.0), ('сегодняшний репутация подонок', 9.0)]\n",
      "Номер текста:  1\n",
      "[('интервью киреев газета', 9.0), ('научиться отличать добро', 9.0), ('глобальный смысл искусство', 8.5), ('стать этакий гуру', 8.5), ('учить человек жизнь', 8.5)]\n",
      "Номер текста:  2\n",
      "[('рамка мировой турне', 9.0), ('бессмысленно озвучивать остальной', 9.0), ('вторник выступить', 4.0), ('московский стадион', 4.0), ('втб-арена', 4.0)]\n",
      "Номер текста:  3\n",
      "[('пресс-релиз сказать', 9.0), ('широкий круг зритель', 9.0), ('высокостатусный таинственный незнакомка', 9.0), ('любимица минкульт рф', 9.0), ('самый знойный блондинка', 9.0)]\n",
      "Номер текста:  4\n",
      "[('чей становление прийтись', 9.0), ('угроза общественный благополучие', 9.0), ('копеечка система здравоохранение', 9.0), ('высокий уровень холестерин', 9.0), ('спектр поведенческий расстройство', 9.0)]\n"
     ]
    }
   ],
   "source": [
    "for _id, text in enumerate(texts):\n",
    "    print(\"Номер текста: \", _id)\n",
    "    result = rake.run(normalize_text_simple_word_tokenize(text), maxWords=3, minFrequency=1)\n",
    "    print(sorted(result, key=lambda x: x[1], reverse=True)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стало хуже. Без лемматизации?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Номер текста:  0\n",
      "[('перечень музыкальный произведение', 9.0), ('великий отечественный война', 9.0), ('культурный норматив школьник', 9.0), ('подрастать поколение макаре', 9.0), ('сегодняшний репутация подонок', 9.0)]\n",
      "Номер текста:  1\n",
      "[('интервью киреев газета', 9.0), ('научиться отличать добро', 9.0), ('глобальный смысл искусство', 8.5), ('стать этакий гуру', 8.5), ('учить человек жизнь', 8.5)]\n",
      "Номер текста:  2\n",
      "[('рамка мировой турне', 9.0), ('бессмысленно озвучивать остальной', 9.0), ('вторник выступить', 4.0), ('московский стадион', 4.0), ('втб-арена', 4.0)]\n",
      "Номер текста:  3\n",
      "[('пресс-релиз сказать', 9.0), ('широкий круг зритель', 9.0), ('высокостатусный таинственный незнакомка', 9.0), ('любимица минкульт рф', 9.0), ('самый знойный блондинка', 9.0)]\n",
      "Номер текста:  4\n",
      "[('чей становление прийтись', 9.0), ('угроза общественный благополучие', 9.0), ('копеечка система здравоохранение', 9.0), ('высокий уровень холестерин', 9.0), ('спектр поведенческий расстройство', 9.0)]\n"
     ]
    }
   ],
   "source": [
    "for _id, text in enumerate(texts):\n",
    "    print(\"Номер текста: \", _id)\n",
    "    result = rake.run(normalize_text_simple_word_tokenize(text), maxWords=3, minFrequency=1)\n",
    "    print(sorted(result, key=lambda x: x[1], reverse=True)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Примерно так же плохо, как со вторым токенизатором. NLTK победил."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization import keywords as kw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Номер текста:  0\n",
      "[('queen', 0.19715471893876976), ('перечень произведение', 0.15815450582858875), ('песнь', 0.14899931091617613), ('nirvana', 0.14011258213601038), ('мединскии', 0.13867923412570002)]\n",
      "Номер текста:  1\n",
      "[('русскии', 0.21301988402043467), ('это', 0.21034994525180167), ('человек', 0.16469527233613862), ('выставка художник владимир киреев', 0.1486620212945899), ('художественныи', 0.13744413763348381)]\n",
      "Номер текста:  2\n",
      "[('scorpions', 0.20424469698559147), ('арен задержать', 0.17803276044910793), ('московскии', 0.17345960559635792), ('втб', 0.1700592017740677), ('концерт группа', 0.15065155780430467)]\n",
      "Номер текста:  3\n",
      "[('год', 0.1713696625003155), ('москва', 0.1591951348039235), ('работа', 0.14725844670832733), ('современныи искусство', 0.13204851077891744), ('выставка око представить произведение художник', 0.11999371695158326)]\n",
      "Номер текста:  4\n",
      "[('поколение', 0.23582193879266028), ('год миллениал человек', 0.20379251240160068), ('экономическии', 0.17253197252580932), ('рост', 0.16675938029309284), ('это', 0.1609619377041676)]\n"
     ]
    }
   ],
   "source": [
    "textrank_result = []\n",
    "for i, text in enumerate(get_clear_texts(corpus, del_stopwords=True)):\n",
    "    print(\"Номер текста: \", i)\n",
    "    result = kw(text, pos_filter=[], scores=True)[:5]\n",
    "    textrank_result.append({i[0] for i in result})\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А стоп-слова-то почистились, откуда взялось это?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'это' in set(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А нету в nltk такого стоп-слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка точности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def precision(pred, true):\n",
    "    try:\n",
    "        p = len(pred & true) / len(pred)\n",
    "    except ZeroDivisionError:\n",
    "        p = 0\n",
    "    return p\n",
    "    \n",
    "def recall(pred, true):\n",
    "    try:\n",
    "        r = len(pred & true) / len(true)\n",
    "    except ZeroDivisionError:\n",
    "        r = 0\n",
    "    return r\n",
    "\n",
    "def f_measure(p, r):\n",
    "    if p == 0 or r == 0:\n",
    "        return 0\n",
    "    return 2 * p * r / (p + r) \n",
    "\n",
    "def estimate(all_pred, all_true):\n",
    "    res = []\n",
    "    for pred, true in zip(all_pred, all_true):\n",
    "        p = precision(pred, true)\n",
    "        r = recall(pred, true)\n",
    "        res.append([p, r, f_measure(p, r)])\n",
    "    return np.mean(np.array(res), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04      , 0.03333333, 0.03636364])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text_rank\n",
    "estimate(textrank_result, all_true_their)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08      , 0.1       , 0.08636364])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate(textrank_result, all_true_mine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02863636, 0.19166667, 0.04977926])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rake\n",
    "estimate(rake_result, all_true_their)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02363636, 0.15666667, 0.04093919])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate(rake_result, all_true_mine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2       , 0.2       , 0.19234654])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tfidf\n",
    "estimate(tfidf_result, all_true_their)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2       , 0.22888889, 0.20493506])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate(tfidf_result, all_true_mine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все очень плохо. Видимо, я плохо подобрала корпус, увы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Бонус\n",
    "\n",
    "Попробуем спасти положение, вытащив именные группы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse import DependencyGraph\n",
    "\n",
    "def get_trees(path):\n",
    "    trees = []\n",
    "\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        parsed_sents = f.read().split('\\n\\n')[1:]\n",
    "\n",
    "    for sent in parsed_sents:\n",
    "        tree = [line for line in sent.split('\\n') if line and line[0] != '#']\n",
    "        trees.append('\\n'.join(tree))\n",
    "    \n",
    "    return trees\n",
    "\n",
    "_FILTER_RELS = ['punct', 'conj', 'parataxis']\n",
    "def get_subtree(nodes, node):\n",
    "    if not nodes[node]['deps']:\n",
    "        return [node]\n",
    "    else:\n",
    "        return [node] + [get_subtree(nodes, dep) for rel in nodes[node]['deps'] \n",
    "                         if rel not in _FILTER_RELS\n",
    "                         for dep in nodes[node]['deps'][rel]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    flat = []\n",
    "    for el in l:\n",
    "        if not isinstance(el, list):\n",
    "            flat.append(el)\n",
    "        else:\n",
    "            flat += flatten(el)\n",
    "    return flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nps(trees):\n",
    "    result = []\n",
    "    for t in trees:\n",
    "        np_list = []\n",
    "        g = DependencyGraph(t, top_relation_label='root')\n",
    "        for n in g.nodes:\n",
    "            if g.nodes[n]['ctag'] == 'NOUN':\n",
    "                np = list(sorted(flatten(get_subtree(g.nodes, n))))\n",
    "                np_list.append(\n",
    "                    ' '.join([g.nodes[i]['word'] for i in np])\n",
    "                )\n",
    "        result.append(np_list)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "conllu_corpus = [i for i in os.listdir(\"corpus\") if i.endswith('.conllu')]\n",
    "forest = []\n",
    "corp_nps = \n",
    "for conll in conllu_corpus:\n",
    "    trees = get_trees(os.path.join(\"corpus\", conll))\n",
    "    npr = get_nps(trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Министерство культуры Российской Федерации', 'культуры Российской Федерации', 'перечень произведений по литературе', 'произведений по литературе', 'по литературе', 'изобразительному искусству', 'кинематографу', 'и музыке с которыми рекомендуется ознакомиться школьникам', 'школьникам'], ['В перечень музыкальных произведений с которыми надо ознакомиться учащимся 9 11 классов', 'музыкальных произведений с которыми надо ознакомиться учащимся 9 11 классов', '9 11 классов', 'хиты Queen Bohemian rhapsody'], ['песни Дунаевского из кинофильма Весёлые ребята', 'из кинофильма Весёлые ребята', 'Весёлые ребята', 'песни о Великой Отечественной войне', 'о Великой Отечественной войне', 'Марионетки Машины времени', 'и Солнечный остров', 'Машины времени', 'времени', 'Под небом голубым Гребенщикова', 'Перемен группы Кино Наутилуса Помпилиуса', 'группы Кино Наутилуса Помпилиуса', 'и Скованные одной цепью'], ['Личные музыкальные вкусы Владимира Мединского заместитель Мединского отвечающая за Культурный норматив школьника', 'заместитель Мединского отвечающая за Культурный норматив школьника', 'за Культурный норматив школьника', 'школьника'], [], ['Зачем подрастающему поколению Макаревич с его скоротечной фигой в кармане', 'с его скоротечной фигой в кармане', 'в кармане', 'и сегодняшней репутацией подонка', 'подонка'], ['Какого чёрта БГ Владимир Семёнович', 'о борьбе', 'о любви'], ['упор на Queen'], [], ['Битлы'], ['гранж'], ['минкультовцам', 'нормативы'], ['Тальков'], ['Кинчев'], [], ['Скорпы'], ['Ганзы'], ['Дорзы'], ['Перплы'], ['Купер'], ['Лучшие произведения советской эстрады', 'советской эстрады'], [], []]\n"
     ]
    }
   ],
   "source": [
    "print(get_nps(trees))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
